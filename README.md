# ğŸ“ˆ Real-Time Stock Data Streaming with Kafka

## ğŸš€ Project Overview

This project simulates **real-time stock price data streaming** using **Apache Kafka (locally installed)**. It fetches intraday stock data from a public API, publishes it to a Kafka topic, and consumes the messages to upload them as **Parquet files** into an **Amazon S3** bucket using `boto3`.

---

## ğŸ”§ Architecture Diagram

![Architecture Diagram](./architecture/real-time-stock-data-streaming-using-kafka.png)

This diagram illustrates the end-to-end flow of the real-time stock data processing pipeline.

---

## ğŸ§° Tech Stack

- **Language:** Python
- **Streaming Platform:** Apache Kafka (Local)
- **Cloud Storage:** AWS S3
- **Libraries:**
  - `kafka-python`
  - `requests`
  - `boto3`
  - `pandas`
  - `pyarrow`

---

## ğŸ“¦ Project Structure

```
01-stock-data-streaming-kafka/
scripts/
â”‚   â””â”€â”€ producer.py                  # Streams stock data to Kafka
â”‚   â””â”€â”€ consumer.py                  # Consumes Kafka messages and writes to S3
â”‚   â””â”€â”€ utils/
â”‚     â””â”€â”€ config_loader.py           # Loads config from JSON
â”œâ”€â”€ config/
â”‚   â””â”€â”€ app_config.yaml              # API, Kafka, and AWS configurations
â””â”€â”€ README.md                        # Project documentation
â””â”€â”€ .env                             # Credential
```

---

## âš™ï¸ Configuration

All parameters are handled through `config/app_config.yaml`.

---

## ğŸš€ Kafka Producer

**What it does:**

- Builds API URL for each stock symbol
- Fetches intraday stock data using Alpha Vantage API
- Sends JSON response to Kafka topic: `stock-data-stream`
- Limits to 2 batches per symbol (for testing/demo)
- Sends one record every 30 seconds

> âœ… Uses `kafka-python` to serialize and push data

---

## ğŸ› Kafka Consumer

**What it does:**

- Listens to `stock-data-stream` Kafka topic
- Buffers records up to 1000 messages
- Converts the batch to a Parquet file using PyArrow
- Uploads to AWS S3 (`stock_batch_<id>_<timestamp>.parquet`)

---

## ğŸ“ Sample Output Structure in S3

```
s3://<bucket-name>/intraday_data/stock_batch_1_1722243172.parquet
```

---

## ğŸ—ƒï¸Sample Data Format (from Producer)

```json
{
  "Meta Data": {
    "1. Information": "Intraday (5min) prices",
    "2. Symbol": "AAPL"
  },
  "Time Series (5min)": {
    "2025-07-29 11:30:00": {
      "1. open": "196.83",
      "2. high": "197.39",
      "3. low": "196.75",
      "4. close": "197.32",
      "5. volume": "1034623"
    }
  }
}
```

---

## â–¶ï¸ How to Run (Local Environment)

### âœ… Prerequisites

- Apache Kafka and Zookeeper running locally
  - You can start them using:
    ```bash
    bin/zookeeper-server-start.sh config/zookeeper.properties
    bin/kafka-server-start.sh config/server.properties
    ```
- Create the Kafka topic:
  ```bash
  bin/kafka-topics.sh --create --topic stock-data-stream --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
  ```
- Python 3.x environment
- Install dependencies:
  ```bash
  pip install kafka-python boto3 requests pandas pyarrow
  ```

### ğŸƒ Run the Producer

```bash
python producer.py
```

### ğŸƒ Run the Consumer

```bash
python consumer.py
```

---

## ğŸ§ Notes & Learnings

- Shows how to implement a real-time pipeline using Kafka
- Hands-on with streaming, batch buffering, and cloud storage
- PyArrow is used for efficient Parquet writing in-memory

---

## ğŸŒ± Future Enhancements

- Normalize nested JSON (e.g., flatten `Time Series`)
- Add schema validation before uploading
- Enhance retry logic and exception handling
- Integrate with Athena/QuickSight for querying

---
